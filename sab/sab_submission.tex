\documentclass{llncs}

\usepackage{graphicx}                                        % for eps, pdf, jpeg, png and tif graphics
\usepackage{amsmath}                                         % for align
\usepackage{amssymb}                                         % for >= and <= signs
\usepackage{xfrac}                                           % for nice x/y fractions (sfrac)
\usepackage{tikz}
\usepackage{amsbsy}                     % for boldsymbol
\usepackage{upgreek}

\mathchardef\mhyp="2D    % define math mode hyphen

\allowdisplaybreaks[4]

\title{Deep feedback learning}

\author{Bernd Porr \and Paul Miller}

\institute{Glasgow Neuro, bernd,paul@glasgowneuro.tech}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
When an agent acts in its environment, its actions in turn will
change its sensor inputs which in turn will cause new actions, or in
short: the agent acts in a closed-loop system. In the simplest case this
is a reactive system where the agent encounters threats or rewards, and
then acts accordingly. Both threats and rewards are unpredictable events
where the agent needs to react as quickly as possible. These have been
called in the past ``disturbances'' or ``perturbations'' because they force
the agent to act at an unpredictable moment in time. Importantly,these
occur at the \textsl{input} of the agent and, thus, a closed loop system
performs ``input control'' -- in contrast to a pattern recognition system
which performs ``output control''.

The above scenario has a major drawback that the reactions are always
too late.  It would be safer for an agent to learn to predict these
disturbances from other cues, and so adaptive behaviour becomes
relevant. At the moment of the disturbance it's already too late - the
tiger has already attacked, or the food has been found by pure
coincidence. However, one can then look back in time and determine
which input signals could have been used to predict this unexpected
threat or reward \cite{Sutton98,Woergoetter2005,PorrNecoInvco2003}.

Adaptive controllers for this kind of learning are either
correlation-based \cite{PorrNecoISO2003,Verschure91} or state
space-based \cite{Dayan1992,Sutton98}. The state space-based ones
have become very powerful because they can be drastically enhanced by
a deep learning network. However, their main drawback is that they are
slow and the state space approach limits their applicability to real
world problems. In addition the deep learning architecture requires
backprop, which contradicts the feedforward nature of
biologically-realistic networks, and can only be justified in special
cases, e.g. by precisely tuned gating and/or feedback in very simple
scenarios. On the other hand correlation based methods can be very
fast, but so far it has been difficult to use them on deep networks
with many layers.  These correlation based methods use ``input
control'', which means that the error signal is fed into the network
at its inputs. This is much more compatible with biology, which
requires a network that propagates errors in a feedforward fashion
instead in the backprop fashion.

How could an error propagated through a network in a forward fashion? Here we can
take inspiration from how errors are transmitted in backprop. 
In essence the hidden layers receive a \textsl{weighted sum} of the errors from the
previous layers which makes intuitive sense: the strongest weights
contribute the most to the error. Imagine we take a similar approach using forward propagation: the error introduced, at the input, will have its strongest
impact via the weights which are high. So, consequently we propagate the error
in a forward fashion in the same way as done in backprop. Eventually the agent
will make an action and this will feedback to the input of the agent, which
in turn then corrects the weights in all layers.

In this paper we present a novel deep network algorithm for closed loop systems
which we call ``Deep Feedback Learning'' (DFL). This algorithm marries the
ideas for correlation based closed loop learning with those from deep learning,
by turning deep learning from a backprop-based algorithm to a forward-prop
one. We demonstrate that this works with the use of two scenarios: a driving task and a 1st-person shooter.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.75\columnwidth]{closed_loop}
  \caption{A closed loop system with a setpoint $SP$, transfer function $H$ and the
    environment $P_0$ which needs to work against unpredictable disturbances $D$.
    The error signal generated tunes a deep neuronal network, which has inputs
    $v_j$ that predict the disturbances. The network tries to pre-empt these
    disturbances and generate an appropriate action $v_k$.
    \label{closed_loop}}
\end{figure}

\section{Closed loop learning}

Deep feedback learning operates in a closed loop scenario. Before we describe
the actual algorithm, we need to place it in a closed loop context. Fig.~\ref{closed_loop} shows the whole closed loop system with the deep
feedback learning as a black box for now. The main idea is that we have a fixed
closed loop which is able to fend off disturbances, such as an unexpected bend on a road or the sudden appearance of an enemy. This fixed loop then
takes appropriate action to solve this disturbance, e.g. steering the car along
the road or aiming towards, and then shooting the enemy. In formal terms we have a setpoint $SP$ which
compares the input of organism to a desired input. If that input deviates from the
setpoint an action is generated with the transfer function $H_0$. This action
then eliminates the disturbance $D$ and arrives via the environmental
transfer function $P_0$ at the input again; thus, the loop is closed. However,
we are not so much interested in the particular design of the the closed loop
but that it generates an \textsl{error signal}. This error signal is non-zero
if a disturbance has happened. This error signal can now be used to tune our
deep feedback learning network.

The deep feedback learning network receives additional inputs which are able
to predict the disturbance, and thus prevent the trigger of the feedback
loop. These additional inputs are provided via the transfer function $P_1$
and represent the disturbance in a filtered form. For example a video camera
can provide images of the road ahead or that of an enemy. Deep feedback
learning has the task to take the error signal and tune its network
to generate an action to minimise the error. In the next section
we describe the deep feedback learning and how this can
compute the appropriate output. Note that deep feedback learning receives
its feedback via the environment, so that it only knows if it has been
successful once its actions have travelled through the environment. Thus learning
evaluates if certain inputs to its network can be used to generate
appropriate actions and these are then slowly transformed into actions.
For that reason the error signal is propagated in a \textsl{forward} fashion
through the network.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\columnwidth]{netw_together}
  \caption{A) Network overview. Except for the input layer
    every neuron is a composite neuron with an activation $v$ and
    and an error term $e$. These are propagated through the network
    in a weighted fashion in parallel.
B) Computation in a single composite cell.
    The presynaptic activities $v_k$ and error signals $e_k$ are used
    to perform correlation based learning and change the weight $w_{kj}$
    which weights both the activity and the error towards the next
    layer.\label{netw_together}}
\end{figure}


\section{Deep feedback learning}
We define a network with an input layer, hidden units and an output
layer which can all have different numbers of neurons (see
Fig~\ref{netw_together}A). In contrast to traditional
networks, every layer (except for the input layer) consists of two
summation nodes: the actual activity and an error signal. These
are processed in two parallel streams.

Let us first focus on the calculation of the network activity. We define a multi-layered network, where every neuron is a standard computational unit that calculates
a weighted sum of its inputs and then applies an activation
function:
\begin{equation}
  v_k = \Theta\left( \sum_i w_{jk} v_{j} \right) \label{act_sum}
\end{equation}
where the activity flows from neurons in layer $v_j$ to neurons in layer $v_k$
and so on. The activities are weighted by the weights $w_{jk}$
in a standard fashion.

The weight change is then calculated in a semi Hebbian fashion:
\begin{equation}
  w_{jk} = w_{jk} + \gamma v_j * e_k
\end{equation}
where $v_j$ is the presynaptic activity and $e_k$ is the error signal
attached to the postsynaptic neuron so that the correlation is
calculated between the input signals and the error signals. This is similar
to Hebbian learning but here the presynaptic term is the activity
and postsynaptic one is the error signal.

The error signal propagation needs to be described in more detail. As outlined above, the error
signal emerges from the feedback loop and is then injected into the
network at its 1st hidden layer as the ``postsynaptic'' activity.
Since the 1st hidden layer directly receives its error signal this
can be computed instantly.

For the deeper layers the error signal is computed as a weighted
sum of the error signals from the previous layer:
\begin{equation}
  e_k = \frac{\left( \sum_j w_{jk} e_{j} \right) \Theta^\prime (v_k) }{\sqrt{\sum_j w_{jk}}}
\end{equation}
where the $\Theta^\prime (v_k)$ is the derivative of the activation
function $\Theta(v_k)$.

The computations performed in each layer are shown in
Fig~\ref{netw_together}B where we see that the activity $v_j$ is
weighted by $w_{jk}$ and then summed in the next
layer. The same happens for the error signal $v_j$ which is also
weighted by $w_{jk}$. Remember that for the 1st hidden layer, this
error signal is just the injected error signal from the feedback loop
and is not the weighted sum.

Learning is then performed in three steps: first the activity is propagated through
the network, then the error signal is propagated via the same mechanism, and finally the
weights are adjusted. Thus, both the error
signal and the activity is propagated in a forward fashion. 
Learning itself is ``Hebbian'' but operates by correlating the actual activation
and the error signal. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=\columnwidth]{linefollower_robot_playground}
  \caption{A) Robot setup. The robot is simulated with a updated
    version of enki for QT5 (https://github.com/berndporr/enki)
    where the line follower is using just the ground sensors of the
    robot to create the error signal and the predictive signals ($v_j$)
    for DFL. The robot has two wheels whose speed is controlled
    by the feedback control and DFL.
    B) The line following scenario used for the simulations. The robot
    attempts to drive along the line and is reversed at the end of the
    line and the drives back. If it hits the boundaries of the playground
    it is also turned around.
    \label{linefollower_robot_playground}}
\end{figure}




\section{Line follower}
In order to demonstrate DFL we need a simple closed loop scenario which can be
improved with the help of an adaptive network. 
Fig.~\ref{linefollower_robot_playground} shows a simple line following robot which has the task of following the line depicted in
Fig.~\ref{linefollower_robot_playground}B to the end, where it reverses and drives back, and so on. The 4 ground sensors
in Fig.~\ref{linefollower_robot_playground}A fixme??? right to both sides of the robot create
an error signal:
\begin{equation}
\mathrm{error} = (g_{l_1}+2 g_{l_2})-(g_{r_1}+2 g_{r_2}) \label{line_error}
\end{equation}
this error directly creates a steering reaction from the fixed feedback loop by controlling the speed of the wheels. Ignoring the contribution of our DFL network this yields: $\mathrm{leftSpeed} = s_0 + g error$ for the
left wheel and $\mathrm{rightSpeed} = s_0 - g error$ for the right wheel,
where $s_0$ is the baseline speed of the robot and $g$ the feedback gain. We now
add the deep feedback learning circuit. 

The DFL learning circuit uses the predictive ground sensors, such that
their outputs feed into the $v_j$ of the input layer (see
Eq.~\ref{act_sum}) after having been filtered bbb fixme Filterbank
bbb. We have two rows of sensors, one directly in front of the robot
and one which looks further ahead.

The output layer of our deep feedback learner consists of 6 neurons with activations
$v_k$ ($k=0 \ldots 5$) 
- these can be seen as soft
decision-making units where 3 of them determine the change of speed of
the right wheel, and 3 the speed of left wheel. This leads to the final formulas for the motor outputs:
\begin{eqnarray}
  \mathrm{leftSpeed} &=& s_0 + \underbrace{g\, \mathrm{error} + \left( 50 v_0 + 10 v_1 + 2 v_2 \right)}_{v_l} \\
                     &=& s_0 + v_l \\
  \mathrm{rightSpeed} &=& s_0 - \underbrace{g\, \mathrm{error} + \left( 50 v_3 + 10 v_4 + 2 v_5 \right)}_{v_r} \\
                     &=& s_0 - v_r
\end{eqnarray}
where $v_0, \ldots, v_5$ are the 6 outputs from the DFL network.
Note that neither inputs nor outputs are organised in a topographically
meangingful way. The network must discover from the error signals
which sensor inputs $v_k$ will lead eventually to appropriate steering actions $v_j$.

For performance criterion we use the squared average of the error from Eq.~\ref{line_error}:
\begin{equation}
  \mathrm{error}_\mathrm{avg} =  \mathrm{error}_\mathrm{avg} + 0.001 (\mathrm{error} - \mathrm{error}_\mathrm{avg}) 
\end{equation}
\begin{equation}
  \mathrm{error}_\mathrm{sq} =  \mathrm{error}_\mathrm{avg}^2 \label{line_sqerr}
\end{equation}
Remember that the learning tries to minimise the average error so that $\mathrm{error}_\mathrm{avg}$
should reach zero in an ideal scenario. Realistically it will fluctuate between negative and
postive values because the driving will never be 100\% perfect. By squaring these fluctuations
we have a positive value which should decay and then stabilise at small values.


\begin{figure}[h!]
  \centering
  \includegraphics[width=\columnwidth]{line_results}
  \caption{Results of the linefollowing task. A) shows the robot at
    the very start of the simulation run from time step 0 to 600.
    B) the difference between the robot wheel speeds just for the learned
    actions (i.e. the output of the dfl network): $v_l-v_r$.
    C) the error signal squared: $\mathrm{error}^2$.
    D) simulation run just before the end at 13400 to 14000 time steps.
    E-G show the weights of the different layers. The input neurons are on the x-axis
    and the output neurons are on the y-axis.
    E) the weights of the 1st layer, F) the weights of the 2nd layer and
    G) the weights of the output layer.
        H) shows the euclidean distance of the weights for each layer from their initial starting point.
    \label{line_results}}
\end{figure}



\subsection{Results}
Fig.~\ref{line_results} shows the results of a simulation run. In the panels
A) and D) we see the trajectory of the agent over the course of 11400 time steps 
of learning. While the agent in A) clearly
just follows the reflex reaction, leading to a large deviation from the track,
in D) the agent follows closely the track and the deviation is minimal, thus
learning has been successful. The steering outputs of the DFL network are shown
in B); they learn the steering actions that keep the agent closely on track.
It can be seen that the network clearly slows down in the change of output.
C) shows the squared error quickly dropping to near zero, leaving only small 
components remaining. E) is the final weight matrix after learning of the 1st layer, 
which correlates the error with the two rows of
predictive ground sensors $v_j$. F) is the weight matrix of the hidden layer and G)
the weight matrix of the output layer. H) shows the eucledian distance of the
different layers from their starting position. Remember that learning is
always on and the learning rate is set to 0.0001 for the single run. See appendix
for the other parameters.

Let us compare the trajectories at the start of learning. Figure
Fig.~\ref{line_results}A shows the robot right at the start. It locks
onto the track and then drives it back and forth with jerky
movements. After $11,400$ time steps learning has turned the erratic
driving into a smooth behaviour where the robot deviates only
minimally from the track (Fig.~\ref{line_results}D). This smooth
steering behaviour is generated by the DFL network output 
shown in Fig.~\ref{line_results}B (net steering angle is shown).

The network learns to use the predictive signals from the sensors in
front of the robot to generate its steering output.  This steering 
output slowly becomes stronger and
then stabilises. Note that this is not
measured in degrees but rather a value that is added and subtracted from the
speed of the robot wheels.

The squared average error in Fig.~\ref{line_results}C slowly decays
leaving only small spikes remaining. This is mainly because
the robot is not able to learn one of the steeper bends (see
coordinate 120x50 in D); this causes a spike in the error and
then the robot overcompensates, causing learning in the
other direction and so on.

The weights of the various layers are shown in
Fig.~\ref{line_results}E-G.  The weights in the input layer
(Fig.~\ref{line_results}E) show a slow gradation  from left to right as
expected. Recall that there are two rows of predictive ground
sensors in front of the robot which cause two different weight maps
which can clearly be seen. The inputs 0 to 14 correspond to the near
ground sensor, and 15 to 31 the far sensor which looks further ahead, 
helping the robot predict bends better. These feed then into the 
hidden layer Fig.~\ref{line_results}F
and from there into the output layer Fig.~\ref{line_results}G.

The weight development overall per layer over the time of learning
is shown in Fig.~\ref{line_results}H. It's interesting to note
that the input layer learns fastest and also its weights converge
quickest while the output layer learns at the slowest rate. Recall 
that the error signal is weighted by the weights for the deeper
layers, but also normalised so that learning happens at about the
same speed in every layer. However, the deeper layers show more
of a an exponential growths. The weights grow at a slower
and slower pace but still continue because the robot is not completely
able to avoid its reflex and probably because the system is non-linear.
However, the error is very low even when the weights still change
substantially so that one could switch off learning if early stability
is required.



\begin{figure}[h!]
  \centering
  \includegraphics[width=\columnwidth]{line_stats}
  \caption{Statistics of the line following task. A) shows the relation between
    simulation steps against learning rate. For every learning rate two runs have
    been conducted with different random number seeds to test the dependence on the
    initialisation of the weights. The simulation was marked successful if the
    squared error Eq.~\ref{line_sqerr} stayed below $10^{-6}$ for more than $2500$
    timesteps. The simulation was aborted after $200,000$ time steps if this criterion
    hasn't been reached. Between the dotted lines every run has been successful.
    B) shows the final squared error for different learning rates and again between
    the dotted lines we have runs which resulted in low squared errors.
    \label{line_stats}}
\end{figure}

We have run statistics of the line follower where we varied the learning
rate over 4 orders of magnitudes and evaluated how long it takes to stay
below a certain error threshold (Fig.~\ref{line_stats}A) for a specified
time and the resulting squared error for these runs (Fig.~\ref{line_stats}B).
We see that the time to reach the criterion decreases with higher learning
rates and is stable between learning rates of approx $10^{-5}$ and $10^{-4}$,
i.e. about one order of magnitude. Each simulation was run twice with different random seeds to test for initialisation effects. It can be seen that in the stable region this results
in different times till success, whereas outwith the stable region about
half of the runs fail. For lower learning rates the trial might still become successful
in the end which, e.g. Fig.~\ref{line_stats}B shows only three runs with low learning rates not converging. At low learning rates the robot sometimes learns essentially
``open loop'' with the error signal having very little impact, and learning
can drift very slowly in the wrong direction only to be corrected later.
At very high learning rates the predictive learning turns into one shot
learning, which might learn accidentially learn the wrong behaviour. This is
then detrimental in numerous cases. In the optimal regime (between the dotted lines)
learning always reaches low error values. Overall it's interesting that lower
learning rates aren't neccessarily better, but rather that intermediate learning rates
perform best as they combine both, learning fast but slow enough so that the error
signal can correct mistakes.




\section{Shooter game}

In this scenario, we try to learn to play a first-person shooter purely from visual inputs. We use the Vizdoom (http://vizdoom.cs.put.edu.pl/) environment for this purpose, and train a controller to play against a single pretrained bot from Intel that ran in the Vizdoom 2016 competition. The setup was as follows:

Our bot’s only actions were to rotate in the plane, and shoot. We did also experiment with having our bot move around, but it is far more difficult to get effective learning trials. In the majority of cases, our bot would almost never see the enemy as it was facing towards the wall. We tried two different input representations - ‘flat’ input image vs assigning a spatial receptive field for each hidden unit. We found little difference in learning, so the flat form is reported here. We also experimented with a variety of temporal filters on the inputs, but found that the best results came from having no filters at all. We report the filterless version here. We created a custom Doom map that randomised spawn positions, so as to help generalise across visual inputs.

The images returned from Vizdoom are RGB. We rendered the enemy in blue, and formed a reflex signal by finding the bounding box of the pixels within a defined similarity of that colour. Note that the reflex is inherently noisy, as other events in the game are also rendered blue (e.g. the ‘flashes’ that happen when an ammo box respawns). The reflex also fails at times when the enemy is too distant or too close to the camera. For learning, we only supply the network with the greyscale image, so it is forced to discover purely spatial cues. The reflex is computed relative to the image centre, so a negative value implies the enemy is on the left. Shooting behaviour is entirely hardwired: if an enemy is detected within a threshold of the image centre, the bot fires. Instead of separate outputs for left and right, we have a single value produced by 3 neurons acting at different sensitivites:
\begin{equation}
\mathrm{rotation} = g_{err}\, \mathrm{error} + g_{net} \left( 10 v_0 + 3 v_1 + v_2 \right)
\end{equation}
where $v_0, \ldots, v_2$ are the network outputs.



\subsection{Results}

As before, we see that the controller outputs steadily increase over time. With a high value of $g_{net}$, the bot can make very rapid aiming movements. This has the advantage that the error can in principle be reduced very quickly; it also causes the bot to sometimes make large rotations even when the enemy is out of the field of view, which helps with exploration. On the other hand, it can cause the aiming to overshoot and oscillate around the target. 

Unlike the Line follower, in the shooting scenario it is not possible to drive the error to zero, as there are discontinuities when either bot respawns, and so the enemy will often abruptly appear somewhere in the image. A good measure of performance is simply how often our bot is killed vs how often it kills the enemy. In gaming, this is called the kill/death ratio, and we plot some smoothed KD curves in Fig.~\ref{shooter_results}



\subsection{Results}


\section{Discussion}
We have shown that a network which propagates its errors in a forward
fashion from its inputs to its outputs is able to solve closed loop
learning tasks. We have demonstrated this in both a first person
shooter and a driving scenario.

Closed loop learning which aims to maximise a function or minimise one
is usually referred to reinforcement learning where an agent learns to
navigate an action space in a way that it maximises its accumulative
reward \cite{Dayan1992}. The main drawback of this approach
is the discrete state space which makes it hard to solve analogue
problems. The overcome this problem closed learning rules using
correlation based techniques \cite{Verschure98summary} were introduced.
These networks perform well in real robot tasks but suffer from
usually simple network architectures. DFL aims to fix this
deficiency by introducing a deep architecture but staying firmly
on correlation based territory.

Plasticity has always been hotly debated in neurophysiology where the
general understanding is that a large postsynaptic Calcium
concentration causes LTP \cite{Malenka99,Bennett2000} and a low causes LTD
\cite{Mulkey1992}. In turn this requires a strong pre-synaptic drive
to achieve a strong postsynaptic activity and with that Ca influx
\cite{Meunier2017}. In mathematical terms this would just lead to
self-amplification of the synaptic weight where strong presynaptic
activity would lead to more postsynaptic activity and in turn stronger
weights and so on. However: what if the learning signal and the actual
activity were transmitted via the same synapse but fundamentally
separated \cite{Lindsay2017} for example by using different
frequencies. High frequency potentials could in this respect cause
plasticity changes while low frequency potentials could propagate
actual activity \cite{Canolty2010}. In this way one could still use
Hebbian plasticity but without any autocorrelation term because the
correlation were to be performed between two different signals.

A different stance about synaptic plasticity (and ultimately how
autonomous agents learn) has been taken by the deep learning community
which recently claimed that error backpropagation is biologically
realistic \cite{Lillicrap2016,Roelfsema2018}. This has been
demonstrated in a network with one hidden layer by introducing a
separate feedback pathway from the output of the network to this
hidden layer. While this shows promising results it still operates in
an open loop fashion (i.e. output control) and thus contradicts the
requirement for an agent to control its inputs which leads to a
discussion about closed loop learning.

Closed loop control or autonomous behaviour can only be understood by
observing the whole loop \cite{Porr2005kyb}. This was already clear with DFL's
predecessor ICO learning \cite{Porr2006ICO} which only makes sense if the
feedback from the environment is taken into account. The same applies
to reinforcement learning \cite{Sutton98}. However, here the
novel aspect is that the more layers DFL has the more it is
remote from the immediate error feedback. This allows for
flexibility which can be tuned by the number of layers and demonstrates
that the agent performs input control. The actions can be wildly
different as long as the error signal can be minimised and will
be more pronounced with more hidden layers. In other words
the more hidden layers we have the more behavioural flexibility
the agent will achieve.

Coming back to the feedback pathways introduced also by the deep
learning community to make a case for backprop. Of course not all
feedback loops traverse through the physical environment but could be
efference copies \cite{Uexkuell26,Graesser86} or gating signals which are offered
to minimise errors. However, even these signals need to be evaluated
at the input of the agent, for example when stabilising an input image
then that is stablised to achieve a certain goal which again is ultimately
an input and not an output.





\bibliographystyle{splncs03}

\bibliography{hebb,ours,embodiment}

\end{document}



\begin{figure}[h!]
  \centering
  \includegraphics[width=\columnwidth]{arena_cct.pdf}
  \caption{A. Overview of the simulation environment. The arena has two markers, labelled R and B (red and blue), within   \label{fig:cct}
    }
\end{figure}

